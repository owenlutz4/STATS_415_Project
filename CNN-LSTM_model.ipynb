{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOXOq5cHVPB0sBRx+zkkOXx",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/owenlutz4/STATS_415_Project/blob/main/CNN-LSTM_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xrKrDLbdeMoD",
        "outputId": "d35c467a-bc52-452c-bb3b-ba1bb266cb6d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ],
      "source": [
        "!pwd"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "# Then change directory to your Drive location\n",
        "%cd /content/drive/MyDrive/human+activity+recognition+using+smartphones_2/UCI_HAR_Dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WqHwpPqJe1nt",
        "outputId": "955bcacb-2fe2-4799-f9db-5695629e3d6a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "/content/drive/MyDrive/human+activity+recognition+using+smartphones_2/UCI_HAR_Dataset\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.metrics import classification_report, confusion_matrix, roc_curve, auc\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from datetime import datetime\n",
        "import os\n",
        "\n",
        "print(\"Loading data...\")\n",
        "\n",
        "try:\n",
        "    # Load training data\n",
        "    X_train = pd.read_csv(\"train/X_train.txt\", sep='\\s+', header=None)\n",
        "    y_train = pd.read_csv(\"train/y_train.txt\", header=None)\n",
        "    subject_train = pd.read_csv(\"train/subject_train.txt\", header=None)\n",
        "\n",
        "    # Load test data\n",
        "    X_test = pd.read_csv(\"test/X_test.txt\", sep='\\s+', header=None)\n",
        "    y_test = pd.read_csv(\"test/y_test.txt\", header=None)\n",
        "    subject_test = pd.read_csv(\"test/subject_test.txt\", header=None)\n",
        "\n",
        "    print(\"Initial data shapes:\")\n",
        "    print(f\"X_train shape: {X_train.shape}\")\n",
        "    print(f\"y_train shape: {y_train.shape}\")\n",
        "    print(f\"X_test shape: {X_test.shape}\")\n",
        "    print(f\"y_test shape: {y_test.shape}\")\n",
        "except Exception as e:\n",
        "    print(\"Failed to load data:\", str(e))\n",
        "    raise\n",
        "\n",
        "# Normalize the data\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# One-hot encode labels\n",
        "encoder = OneHotEncoder(sparse_output=False)\n",
        "y_train_encoded = encoder.fit_transform(y_train)\n",
        "y_test_encoded = encoder.transform(y_test)\n",
        "\n",
        "# Get labels for the stratification\n",
        "y_train_classes = np.argmax(y_train_encoded, axis=1)\n",
        "\n",
        "# Calculate dimensions\n",
        "n_timesteps = 11\n",
        "n_features = 51\n",
        "n_classes = y_train_encoded.shape[1]\n",
        "\n",
        "# Reshape data for CNN-LSTM\n",
        "X_train_reshaped = X_train_scaled.reshape(-1, n_timesteps, n_features)\n",
        "X_test_reshaped = X_test_scaled.reshape(-1, n_timesteps, n_features)\n",
        "\n",
        "def create_model(conv_filters, lstm_units, dropout_rate):\n",
        "    model = tf.keras.Sequential([\n",
        "        tf.keras.layers.Input(shape=(n_timesteps, n_features)),\n",
        "        tf.keras.layers.Conv1D(\n",
        "            filters=conv_filters,\n",
        "            kernel_size=5,\n",
        "            activation='relu',\n",
        "            kernel_regularizer=tf.keras.regularizers.l2(0.0005)\n",
        "        ),\n",
        "        tf.keras.layers.BatchNormalization(),\n",
        "        tf.keras.layers.Dropout(dropout_rate),\n",
        "        tf.keras.layers.MaxPooling1D(pool_size=2),\n",
        "        tf.keras.layers.LSTM(\n",
        "            lstm_units,\n",
        "            return_sequences=False,\n",
        "            kernel_regularizer=tf.keras.regularizers.l2(0.0005),\n",
        "            recurrent_regularizer=tf.keras.regularizers.l2(0.0005)\n",
        "        ),\n",
        "        tf.keras.layers.Dropout(dropout_rate),\n",
        "        tf.keras.layers.Dense(\n",
        "            32,\n",
        "            activation='relu',\n",
        "            kernel_regularizer=tf.keras.regularizers.l2(0.0005)\n",
        "        ),\n",
        "        tf.keras.layers.BatchNormalization(),\n",
        "        tf.keras.layers.Dense(\n",
        "          n_classes,\n",
        "          activation='softmax',\n",
        "          kernel_regularizer=tf.keras.regularizers.l2(0.0005)\n",
        "        )\n",
        "    ])\n",
        "    return model\n",
        "\n",
        "# Define parameter grid\n",
        "param_grid = {\n",
        "    'conv_filters': [32, 64],\n",
        "    'lstm_units': [32, 64],\n",
        "    'dropout_rate': [0.25, 0.35],\n",
        "    'learning_rate': [0.0001, 0.0005],\n",
        "    'batch_size': [64, 128]\n",
        "}\n",
        "\n",
        "def grid_search_cv():\n",
        "    best_accuracy = 0\n",
        "    best_params = {}\n",
        "    results = []\n",
        "\n",
        "    total_combinations = (len(param_grid['conv_filters']) *\n",
        "                        len(param_grid['lstm_units']) *\n",
        "                        len(param_grid['dropout_rate']) *\n",
        "                        len(param_grid['learning_rate']) *\n",
        "                        len(param_grid['batch_size']))\n",
        "\n",
        "    print(f\"Starting grid search with {total_combinations} combinations...\")\n",
        "    combination_count = 0\n",
        "\n",
        "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "    try:\n",
        "        for conv_filters in param_grid['conv_filters']:\n",
        "            for lstm_units in param_grid['lstm_units']:\n",
        "                for dropout_rate in param_grid['dropout_rate']:\n",
        "                    for learning_rate in param_grid['learning_rate']:\n",
        "                        for batch_size in param_grid['batch_size']:\n",
        "                            combination_count += 1\n",
        "                            print(f\"\\nTrying combination {combination_count}/{total_combinations}\")\n",
        "                            print(f\"Parameters: conv_filters={conv_filters}, lstm_units={lstm_units}, \"\n",
        "                                  f\"dropout_rate={dropout_rate}, lr={learning_rate}, batch_size={batch_size}\")\n",
        "\n",
        "                            cv_scores = []\n",
        "\n",
        "                            for fold, (train_idx, val_idx) in enumerate(skf.split(X_train_reshaped, y_train_classes)):\n",
        "                                print(f\"Fold {fold+1}/5\")\n",
        "\n",
        "                                X_fold_train = X_train_reshaped[train_idx]\n",
        "                                y_fold_train = y_train_encoded[train_idx]\n",
        "                                X_fold_val = X_train_reshaped[val_idx]\n",
        "                                y_fold_val = y_train_encoded[val_idx]\n",
        "\n",
        "                                try:\n",
        "                                    model = create_model(conv_filters, lstm_units, dropout_rate)\n",
        "                                    model.compile(\n",
        "                                        optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\n",
        "                                        loss='categorical_crossentropy',\n",
        "                                        metrics=['accuracy']\n",
        "                                    )\n",
        "\n",
        "                                    model.fit(\n",
        "                                        X_fold_train,\n",
        "                                        y_fold_train,\n",
        "                                        epochs=50,\n",
        "                                        batch_size=batch_size,\n",
        "                                        validation_data=(X_fold_val, y_fold_val),\n",
        "                                        callbacks=[\n",
        "                                            tf.keras.callbacks.EarlyStopping(\n",
        "                                                monitor='val_loss',\n",
        "                                                patience=3,\n",
        "                                                restore_best_weights=True,\n",
        "                                                min_delta=0.005\n",
        "                                            )\n",
        "                                        ],\n",
        "                                        verbose=0\n",
        "                                    )\n",
        "\n",
        "                                    _, fold_accuracy = model.evaluate(X_fold_val, y_fold_val, verbose=0)\n",
        "                                    cv_scores.append(fold_accuracy)\n",
        "\n",
        "                                except tf.errors.ResourceExhaustedError:\n",
        "                                    print(f\"Skipping fold {fold+1} due to memory error\")\n",
        "                                    continue\n",
        "                                finally:\n",
        "                                    tf.keras.backend.clear_session()\n",
        "\n",
        "                            if cv_scores:\n",
        "                                mean_cv_accuracy = np.mean(cv_scores)\n",
        "                                std_cv_accuracy = np.std(cv_scores)\n",
        "                                print(f\"\\nMean CV accuracy: {mean_cv_accuracy:.4f} (+/- {std_cv_accuracy:.4f})\")\n",
        "\n",
        "                                results.append({\n",
        "                                    'conv_filters': conv_filters,\n",
        "                                    'lstm_units': lstm_units,\n",
        "                                    'dropout_rate': dropout_rate,\n",
        "                                    'learning_rate': learning_rate,\n",
        "                                    'batch_size': batch_size,\n",
        "                                    'mean_cv_accuracy': mean_cv_accuracy,\n",
        "                                    'std_cv_accuracy': std_cv_accuracy\n",
        "                                })\n",
        "\n",
        "                                if mean_cv_accuracy > best_accuracy:\n",
        "                                    best_accuracy = mean_cv_accuracy\n",
        "                                    best_params = {\n",
        "                                        'conv_filters': conv_filters,\n",
        "                                        'lstm_units': lstm_units,\n",
        "                                        'dropout_rate': dropout_rate,\n",
        "                                        'learning_rate': learning_rate,\n",
        "                                        'batch_size': batch_size\n",
        "                                    }\n",
        "                                    print(f\"\\nNew best accuracy: {best_accuracy:.4f}\")\n",
        "\n",
        "    except KeyboardInterrupt:\n",
        "        print(\"\\nGrid search interrupted by user\")\n",
        "        if best_params:\n",
        "            return best_params, best_accuracy, results\n",
        "        raise\n",
        "\n",
        "    return best_params, best_accuracy, results\n",
        "\n",
        "print(\"Starting grid search with 5-fold cross validation...\")\n",
        "best_params, best_accuracy, all_results = grid_search_cv()\n",
        "\n",
        "print(\"\\nGrid search complete!\")\n",
        "print(f\"Best parameters found:\")\n",
        "print(f\"- Conv filters: {best_params['conv_filters']}\")\n",
        "print(f\"- LSTM units: {best_params['lstm_units']}\")\n",
        "print(f\"- Dropout rate: {best_params['dropout_rate']}\")\n",
        "print(f\"- Learning rate: {best_params['learning_rate']}\")\n",
        "print(f\"- Batch size: {best_params['batch_size']}\")\n",
        "print(f\"Best CV accuracy: {best_accuracy:.4f}\")\n",
        "\n",
        "# Train final model\n",
        "print(\"\\nTraining final model with best parameters...\")\n",
        "final_model = create_model(\n",
        "    best_params['conv_filters'],\n",
        "    best_params['lstm_units'],\n",
        "    best_params['dropout_rate']\n",
        ")\n",
        "\n",
        "final_model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(learning_rate=best_params['learning_rate']),\n",
        "    loss='categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "base_dir = 'model_outputs'\n",
        "output_dir = f'{base_dir}_{timestamp}'\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "final_history = final_model.fit(\n",
        "    X_train_reshaped,\n",
        "    y_train_encoded,\n",
        "    epochs=50,\n",
        "    batch_size=best_params['batch_size'],\n",
        "    validation_split=0.2,\n",
        "    callbacks=[\n",
        "        tf.keras.callbacks.EarlyStopping(\n",
        "            monitor='val_loss',\n",
        "            patience=3,\n",
        "            restore_best_weights=True,\n",
        "            min_delta=0.005\n",
        "        ),\n",
        "        tf.keras.callbacks.ModelCheckpoint(\n",
        "            'best_model.weights.h5',\n",
        "            monitor='val_loss',\n",
        "            save_best_only=True,\n",
        "            mode='min',\n",
        "            save_weights_only=True\n",
        "        )\n",
        "    ],\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "print(\"\\nEvaluating final model...\")\n",
        "test_loss, test_accuracy = final_model.evaluate(X_test_reshaped, y_test_encoded)\n",
        "print(f\"Final Test Accuracy: {test_accuracy:.4f}\")\n",
        "\n",
        "y_pred = final_model.predict(X_test_reshaped)\n",
        "y_pred_classes = np.argmax(y_pred, axis=1)\n",
        "y_test_classes = np.argmax(y_test_encoded, axis=1)\n",
        "\n",
        "activity_labels = [\n",
        "    'WALKING',\n",
        "    'WALKING_UPSTAIRS',\n",
        "    'WALKING_DOWNSTAIRS',\n",
        "    'SITTING',\n",
        "    'STANDING',\n",
        "    'LAYING'\n",
        "]\n",
        "\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test_classes, y_pred_classes, target_names=activity_labels))\n",
        "\n",
        "def plot_confusion_matrix(y_true, y_pred, classes, filename):\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "    plt.figure(figsize=(12, 10))\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "                xticklabels=classes, yticklabels=classes)\n",
        "    plt.title('Confusion Matrix of Human Activities')\n",
        "    plt.ylabel('True Activity')\n",
        "    plt.xlabel('Predicted Activity')\n",
        "    plt.xticks(rotation=45, ha='right')\n",
        "    plt.yticks(rotation=45, ha='right')\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(filename, bbox_inches='tight', dpi=300)\n",
        "    plt.close()\n",
        "\n",
        "plot_confusion_matrix(\n",
        "    y_test_classes,\n",
        "    y_pred_classes,\n",
        "    activity_labels,\n",
        "    os.path.join(output_dir, 'confusion_matrix.png')\n",
        ")\n",
        "\n",
        "# Accuracy History Plot\n",
        "\n",
        "plt.figure(figsize=(12, 4))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(final_history.history['accuracy'], label='Training')\n",
        "plt.plot(final_history.history['val_accuracy'], label='Validation')\n",
        "plt.title('Model Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "\n",
        "# Loss History Plot\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(final_history.history['loss'], label='Training')\n",
        "plt.plot(final_history.history['val_loss'], label='Validation')\n",
        "plt.title('Model Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(os.path.join(output_dir, 'training_history.png'))\n",
        "plt.close()\n",
        "\n",
        "try:\n",
        "    model_path = os.path.join(output_dir, 'final_model.h5')\n",
        "    final_model.save(model_path)\n",
        "    print(f\"Model saved to: {model_path}\")\n",
        "except Exception as e:\n",
        "    print(f\"Error saving model: {str(e)}\")\n",
        "    try:\n",
        "        backup_path = os.path.join(output_dir, 'final_model_backup')\n",
        "        final_model.save(backup_path, save_format='tf')\n",
        "        print(f\"Model saved to backup location: {backup_path}\")\n",
        "    except Exception as e2:\n",
        "        print(f\"Failed to save backup model: {str(e2)}\")\n",
        "\n",
        "results_df = pd.DataFrame(all_results)\n",
        "results_df.to_csv(os.path.join(output_dir, 'grid_search_results.csv'), index=False)\n",
        "\n",
        "print(f\"\\nOutputs saved to: {output_dir}\")\n",
        "\n",
        "def cleanup():\n",
        "   tf.keras.backend.clear_session()\n",
        "   plt.close('all')\n",
        "   if os.path.exists('best_model.weights.h5'):\n",
        "       os.remove('best_model.weights.h5')\n",
        "\n",
        "import atexit\n",
        "atexit.register(cleanup)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "   try:\n",
        "       print(\"Starting training process...\")\n",
        "   except Exception as e:\n",
        "       print(f\"Error in main process: {str(e)}\")\n",
        "   finally:\n",
        "       cleanup()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "QkbA2TGfHqVj",
        "outputId": "a4cd2939-d28d-40bc-d725-96a3a602482d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading data...\n",
            "Initial data shapes:\n",
            "X_train shape: (7352, 561)\n",
            "y_train shape: (7352, 1)\n",
            "X_test shape: (2947, 561)\n",
            "y_test shape: (2947, 1)\n",
            "Starting grid search with 5-fold cross validation...\n",
            "Starting grid search with 32 combinations...\n",
            "\n",
            "Trying combination 1/32\n",
            "Parameters: conv_filters=32, lstm_units=32, dropout_rate=0.3, lr=0.0001, batch_size=64\n",
            "Fold 1/5\n",
            "Fold 2/5\n",
            "Fold 3/5\n",
            "Fold 4/5\n",
            "Fold 5/5\n",
            "\n",
            "Mean CV accuracy: 0.9578 (+/- 0.0085)\n",
            "\n",
            "New best accuracy: 0.9578\n",
            "\n",
            "Trying combination 2/32\n",
            "Parameters: conv_filters=32, lstm_units=32, dropout_rate=0.3, lr=0.0001, batch_size=128\n",
            "Fold 1/5\n",
            "Fold 2/5\n",
            "Fold 3/5\n",
            "Fold 4/5\n",
            "Fold 5/5\n",
            "\n",
            "Mean CV accuracy: 0.9376 (+/- 0.0087)\n",
            "\n",
            "Trying combination 3/32\n",
            "Parameters: conv_filters=32, lstm_units=32, dropout_rate=0.3, lr=0.0005, batch_size=64\n",
            "Fold 1/5\n",
            "Fold 2/5\n",
            "Fold 3/5\n",
            "Fold 4/5\n",
            "Fold 5/5\n",
            "\n",
            "Mean CV accuracy: 0.9725 (+/- 0.0054)\n",
            "\n",
            "New best accuracy: 0.9725\n",
            "\n",
            "Trying combination 4/32\n",
            "Parameters: conv_filters=32, lstm_units=32, dropout_rate=0.3, lr=0.0005, batch_size=128\n",
            "Fold 1/5\n",
            "Fold 2/5\n",
            "Fold 3/5\n",
            "Fold 4/5\n",
            "Fold 5/5\n",
            "\n",
            "Mean CV accuracy: 0.9708 (+/- 0.0055)\n",
            "\n",
            "Trying combination 5/32\n",
            "Parameters: conv_filters=32, lstm_units=32, dropout_rate=0.4, lr=0.0001, batch_size=64\n",
            "Fold 1/5\n",
            "Fold 2/5\n",
            "Fold 3/5\n",
            "Fold 4/5\n",
            "Fold 5/5\n",
            "\n",
            "Mean CV accuracy: 0.9400 (+/- 0.0109)\n",
            "\n",
            "Trying combination 6/32\n",
            "Parameters: conv_filters=32, lstm_units=32, dropout_rate=0.4, lr=0.0001, batch_size=128\n",
            "Fold 1/5\n",
            "Fold 2/5\n",
            "Fold 3/5\n",
            "Fold 4/5\n",
            "Fold 5/5\n",
            "\n",
            "Mean CV accuracy: 0.9255 (+/- 0.0030)\n",
            "\n",
            "Trying combination 7/32\n",
            "Parameters: conv_filters=32, lstm_units=32, dropout_rate=0.4, lr=0.0005, batch_size=64\n",
            "Fold 1/5\n",
            "Fold 2/5\n",
            "Fold 3/5\n",
            "Fold 4/5\n",
            "Fold 5/5\n",
            "\n",
            "Mean CV accuracy: 0.9686 (+/- 0.0042)\n",
            "\n",
            "Trying combination 8/32\n",
            "Parameters: conv_filters=32, lstm_units=32, dropout_rate=0.4, lr=0.0005, batch_size=128\n",
            "Fold 1/5\n",
            "Fold 2/5\n",
            "Fold 3/5\n",
            "Fold 4/5\n",
            "Fold 5/5\n",
            "\n",
            "Mean CV accuracy: 0.9615 (+/- 0.0117)\n",
            "\n",
            "Trying combination 9/32\n",
            "Parameters: conv_filters=32, lstm_units=64, dropout_rate=0.3, lr=0.0001, batch_size=64\n",
            "Fold 1/5\n",
            "Fold 2/5\n",
            "Fold 3/5\n",
            "Fold 4/5\n",
            "Fold 5/5\n",
            "\n",
            "Mean CV accuracy: 0.9597 (+/- 0.0074)\n",
            "\n",
            "Trying combination 10/32\n",
            "Parameters: conv_filters=32, lstm_units=64, dropout_rate=0.3, lr=0.0001, batch_size=128\n",
            "Fold 1/5\n",
            "Fold 2/5\n",
            "Fold 3/5\n",
            "Fold 4/5\n",
            "Fold 5/5\n",
            "\n",
            "Mean CV accuracy: 0.9550 (+/- 0.0049)\n",
            "\n",
            "Trying combination 11/32\n",
            "Parameters: conv_filters=32, lstm_units=64, dropout_rate=0.3, lr=0.0005, batch_size=64\n",
            "Fold 1/5\n",
            "Fold 2/5\n",
            "Fold 3/5\n",
            "Fold 4/5\n",
            "Fold 5/5\n",
            "\n",
            "Mean CV accuracy: 0.9721 (+/- 0.0057)\n",
            "\n",
            "Trying combination 12/32\n",
            "Parameters: conv_filters=32, lstm_units=64, dropout_rate=0.3, lr=0.0005, batch_size=128\n",
            "Fold 1/5\n",
            "Fold 2/5\n",
            "Fold 3/5\n",
            "Fold 4/5\n",
            "Fold 5/5\n",
            "\n",
            "Mean CV accuracy: 0.9718 (+/- 0.0046)\n",
            "\n",
            "Trying combination 13/32\n",
            "Parameters: conv_filters=32, lstm_units=64, dropout_rate=0.4, lr=0.0001, batch_size=64\n",
            "Fold 1/5\n",
            "Fold 2/5\n",
            "Fold 3/5\n",
            "Fold 4/5\n",
            "Fold 5/5\n",
            "\n",
            "Mean CV accuracy: 0.9573 (+/- 0.0052)\n",
            "\n",
            "Trying combination 14/32\n",
            "Parameters: conv_filters=32, lstm_units=64, dropout_rate=0.4, lr=0.0001, batch_size=128\n",
            "Fold 1/5\n",
            "Fold 2/5\n",
            "Fold 3/5\n",
            "Fold 4/5\n",
            "Fold 5/5\n",
            "\n",
            "Mean CV accuracy: 0.9348 (+/- 0.0066)\n",
            "\n",
            "Trying combination 15/32\n",
            "Parameters: conv_filters=32, lstm_units=64, dropout_rate=0.4, lr=0.0005, batch_size=64\n",
            "Fold 1/5\n",
            "Fold 2/5\n",
            "Fold 3/5\n",
            "Fold 4/5\n",
            "Fold 5/5\n",
            "\n",
            "Mean CV accuracy: 0.9693 (+/- 0.0024)\n",
            "\n",
            "Trying combination 16/32\n",
            "Parameters: conv_filters=32, lstm_units=64, dropout_rate=0.4, lr=0.0005, batch_size=128\n",
            "Fold 1/5\n",
            "Fold 2/5\n",
            "Fold 3/5\n",
            "Fold 4/5\n",
            "Fold 5/5\n",
            "\n",
            "Mean CV accuracy: 0.9680 (+/- 0.0037)\n",
            "\n",
            "Trying combination 17/32\n",
            "Parameters: conv_filters=64, lstm_units=32, dropout_rate=0.3, lr=0.0001, batch_size=64\n",
            "Fold 1/5\n",
            "Fold 2/5\n",
            "Fold 3/5\n",
            "Fold 4/5\n",
            "Fold 5/5\n",
            "\n",
            "Mean CV accuracy: 0.9610 (+/- 0.0098)\n",
            "\n",
            "Trying combination 18/32\n",
            "Parameters: conv_filters=64, lstm_units=32, dropout_rate=0.3, lr=0.0001, batch_size=128\n",
            "Fold 1/5\n",
            "Fold 2/5\n",
            "Fold 3/5\n",
            "Fold 4/5\n",
            "Fold 5/5\n",
            "\n",
            "Mean CV accuracy: 0.9539 (+/- 0.0047)\n",
            "\n",
            "Trying combination 19/32\n",
            "Parameters: conv_filters=64, lstm_units=32, dropout_rate=0.3, lr=0.0005, batch_size=64\n",
            "Fold 1/5\n",
            "Fold 2/5\n",
            "Fold 3/5\n",
            "Fold 4/5\n",
            "Fold 5/5\n",
            "\n",
            "Mean CV accuracy: 0.9744 (+/- 0.0038)\n",
            "\n",
            "New best accuracy: 0.9744\n",
            "\n",
            "Trying combination 20/32\n",
            "Parameters: conv_filters=64, lstm_units=32, dropout_rate=0.3, lr=0.0005, batch_size=128\n",
            "Fold 1/5\n",
            "Fold 2/5\n",
            "Fold 3/5\n",
            "Fold 4/5\n",
            "Fold 5/5\n",
            "\n",
            "Mean CV accuracy: 0.9748 (+/- 0.0050)\n",
            "\n",
            "New best accuracy: 0.9748\n",
            "\n",
            "Trying combination 21/32\n",
            "Parameters: conv_filters=64, lstm_units=32, dropout_rate=0.4, lr=0.0001, batch_size=64\n",
            "Fold 1/5\n",
            "Fold 2/5\n",
            "Fold 3/5\n",
            "Fold 4/5\n",
            "Fold 5/5\n",
            "\n",
            "Mean CV accuracy: 0.9542 (+/- 0.0068)\n",
            "\n",
            "Trying combination 22/32\n",
            "Parameters: conv_filters=64, lstm_units=32, dropout_rate=0.4, lr=0.0001, batch_size=128\n",
            "Fold 1/5\n",
            "Fold 2/5\n",
            "Fold 3/5\n",
            "Fold 4/5\n",
            "Fold 5/5\n",
            "\n",
            "Mean CV accuracy: 0.9404 (+/- 0.0086)\n",
            "\n",
            "Trying combination 23/32\n",
            "Parameters: conv_filters=64, lstm_units=32, dropout_rate=0.4, lr=0.0005, batch_size=64\n",
            "Fold 1/5\n",
            "Fold 2/5\n",
            "Fold 3/5\n",
            "Fold 4/5\n",
            "Fold 5/5\n",
            "\n",
            "Mean CV accuracy: 0.9703 (+/- 0.0093)\n",
            "\n",
            "Trying combination 24/32\n",
            "Parameters: conv_filters=64, lstm_units=32, dropout_rate=0.4, lr=0.0005, batch_size=128\n",
            "Fold 1/5\n",
            "Fold 2/5\n",
            "Fold 3/5\n",
            "Fold 4/5\n",
            "Fold 5/5\n",
            "\n",
            "Mean CV accuracy: 0.9682 (+/- 0.0061)\n",
            "\n",
            "Trying combination 25/32\n",
            "Parameters: conv_filters=64, lstm_units=64, dropout_rate=0.3, lr=0.0001, batch_size=64\n",
            "Fold 1/5\n",
            "Fold 2/5\n",
            "Fold 3/5\n",
            "Fold 4/5\n",
            "Fold 5/5\n",
            "\n",
            "Mean CV accuracy: 0.9644 (+/- 0.0036)\n",
            "\n",
            "Trying combination 26/32\n",
            "Parameters: conv_filters=64, lstm_units=64, dropout_rate=0.3, lr=0.0001, batch_size=128\n",
            "Fold 1/5\n",
            "Fold 2/5\n",
            "Fold 3/5\n",
            "Fold 4/5\n",
            "Fold 5/5\n",
            "\n",
            "Mean CV accuracy: 0.9587 (+/- 0.0123)\n",
            "\n",
            "Trying combination 27/32\n",
            "Parameters: conv_filters=64, lstm_units=64, dropout_rate=0.3, lr=0.0005, batch_size=64\n",
            "Fold 1/5\n",
            "Fold 2/5\n",
            "Fold 3/5\n",
            "Fold 4/5\n",
            "Fold 5/5\n",
            "\n",
            "Mean CV accuracy: 0.9767 (+/- 0.0066)\n",
            "\n",
            "New best accuracy: 0.9767\n",
            "\n",
            "Trying combination 28/32\n",
            "Parameters: conv_filters=64, lstm_units=64, dropout_rate=0.3, lr=0.0005, batch_size=128\n",
            "Fold 1/5\n",
            "Fold 2/5\n",
            "Fold 3/5\n",
            "Fold 4/5\n",
            "Fold 5/5\n",
            "\n",
            "Mean CV accuracy: 0.9754 (+/- 0.0046)\n",
            "\n",
            "Trying combination 29/32\n",
            "Parameters: conv_filters=64, lstm_units=64, dropout_rate=0.4, lr=0.0001, batch_size=64\n",
            "Fold 1/5\n",
            "Fold 2/5\n",
            "Fold 3/5\n",
            "Fold 4/5\n",
            "Fold 5/5\n",
            "\n",
            "Mean CV accuracy: 0.9633 (+/- 0.0064)\n",
            "\n",
            "Trying combination 30/32\n",
            "Parameters: conv_filters=64, lstm_units=64, dropout_rate=0.4, lr=0.0001, batch_size=128\n",
            "Fold 1/5\n",
            "Fold 2/5\n",
            "Fold 3/5\n",
            "Fold 4/5\n",
            "Fold 5/5\n",
            "\n",
            "Mean CV accuracy: 0.9547 (+/- 0.0067)\n",
            "\n",
            "Trying combination 31/32\n",
            "Parameters: conv_filters=64, lstm_units=64, dropout_rate=0.4, lr=0.0005, batch_size=64\n",
            "Fold 1/5\n",
            "Fold 2/5\n",
            "Fold 3/5\n",
            "Fold 4/5\n",
            "Fold 5/5\n",
            "\n",
            "Mean CV accuracy: 0.9785 (+/- 0.0035)\n",
            "\n",
            "New best accuracy: 0.9785\n",
            "\n",
            "Trying combination 32/32\n",
            "Parameters: conv_filters=64, lstm_units=64, dropout_rate=0.4, lr=0.0005, batch_size=128\n",
            "Fold 1/5\n",
            "Fold 2/5\n",
            "Fold 3/5\n",
            "Fold 4/5\n",
            "Fold 5/5\n",
            "\n",
            "Mean CV accuracy: 0.9747 (+/- 0.0046)\n",
            "\n",
            "Grid search complete!\n",
            "Best parameters found:\n",
            "- Conv filters: 64\n",
            "- LSTM units: 64\n",
            "- Dropout rate: 0.4\n",
            "- Learning rate: 0.0005\n",
            "- Batch size: 64\n",
            "Best CV accuracy: 0.9785\n",
            "\n",
            "Training final model with best parameters...\n",
            "Epoch 1/50\n",
            "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 16ms/step - accuracy: 0.3897 - loss: 1.6990 - val_accuracy: 0.7492 - val_loss: 1.1793\n",
            "Epoch 2/50\n",
            "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step - accuracy: 0.6813 - loss: 0.9100 - val_accuracy: 0.8334 - val_loss: 0.6969\n",
            "Epoch 3/50\n",
            "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.7932 - loss: 0.6783 - val_accuracy: 0.8586 - val_loss: 0.4656\n",
            "Epoch 4/50\n",
            "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - accuracy: 0.8396 - loss: 0.5468 - val_accuracy: 0.9239 - val_loss: 0.3638\n",
            "Epoch 5/50\n",
            "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 18ms/step - accuracy: 0.8734 - loss: 0.4697 - val_accuracy: 0.9300 - val_loss: 0.3039\n",
            "Epoch 6/50\n",
            "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - accuracy: 0.8932 - loss: 0.4204 - val_accuracy: 0.9293 - val_loss: 0.3041\n",
            "Epoch 7/50\n",
            "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.9223 - loss: 0.3509 - val_accuracy: 0.9422 - val_loss: 0.2766\n",
            "Epoch 8/50\n",
            "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.9292 - loss: 0.3337 - val_accuracy: 0.9409 - val_loss: 0.2816\n",
            "Epoch 9/50\n",
            "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.9345 - loss: 0.3032 - val_accuracy: 0.9429 - val_loss: 0.2777\n",
            "Epoch 10/50\n",
            "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.9451 - loss: 0.2688 - val_accuracy: 0.9497 - val_loss: 0.2486\n",
            "Epoch 11/50\n",
            "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.9476 - loss: 0.2635 - val_accuracy: 0.9341 - val_loss: 0.2871\n",
            "Epoch 12/50\n",
            "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.9535 - loss: 0.2523 - val_accuracy: 0.9477 - val_loss: 0.2464\n",
            "Epoch 13/50\n",
            "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.9570 - loss: 0.2369 - val_accuracy: 0.9538 - val_loss: 0.2512\n",
            "\n",
            "Evaluating final model...\n",
            "\u001b[1m93/93\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9190 - loss: 0.3290\n",
            "Final Test Accuracy: 0.9165\n",
            "\u001b[1m93/93\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step\n",
            "\n",
            "Classification Report:\n",
            "                    precision    recall  f1-score   support\n",
            "\n",
            "           WALKING       0.87      0.99      0.93       496\n",
            "  WALKING_UPSTAIRS       0.90      0.87      0.89       471\n",
            "WALKING_DOWNSTAIRS       0.94      0.83      0.88       420\n",
            "           SITTING       0.94      0.84      0.88       491\n",
            "          STANDING       0.87      0.95      0.91       532\n",
            "            LAYING       1.00      1.00      1.00       537\n",
            "\n",
            "          accuracy                           0.92      2947\n",
            "         macro avg       0.92      0.91      0.91      2947\n",
            "      weighted avg       0.92      0.92      0.92      2947\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model saved to: model_outputs_20241207_065656/final_model.h5\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Outputs saved to: model_outputs_20241207_065656\n",
            "Starting training process...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import psutil\n",
        "import numpy as np\n",
        "import os\n",
        "from datetime import datetime\n",
        "\n",
        "def measure_inference_performance(model, X_test, n_runs=100):\n",
        "    \"\"\"\n",
        "    Measure inference time and memory usage of the model.\n",
        "\n",
        "    Args:\n",
        "        model: Trained TensorFlow model\n",
        "        X_test: Test data\n",
        "        n_runs: Number of inference runs to average over\n",
        "\n",
        "    Returns:\n",
        "        dict: Performance metrics\n",
        "    \"\"\"\n",
        "    # Warm up the model\n",
        "    for _ in range(5):\n",
        "        model.predict(X_test[:1])\n",
        "\n",
        "    # Measure inference time\n",
        "    latencies = []\n",
        "    memory_usage = []\n",
        "\n",
        "    for _ in range(n_runs):\n",
        "        # Single sample inference (real-time scenario)\n",
        "        start_time = time.time()\n",
        "        model.predict(X_test[:1])\n",
        "        latencies.append((time.time() - start_time) * 1000)  # Convert to milliseconds\n",
        "\n",
        "        # Memory usage\n",
        "        process = psutil.Process()\n",
        "        memory_usage.append(process.memory_info().rss / 1024 / 1024)  # Convert to MB\n",
        "\n",
        "    metrics = {\n",
        "        'avg_inference_time_ms': np.mean(latencies),\n",
        "        'std_inference_time_ms': np.std(latencies),\n",
        "        'max_inference_time_ms': np.max(latencies),\n",
        "        'p95_inference_time_ms': np.percentile(latencies, 95),\n",
        "        'avg_memory_usage_mb': np.mean(memory_usage),\n",
        "        'peak_memory_usage_mb': np.max(memory_usage)\n",
        "    }\n",
        "\n",
        "    return metrics\n",
        "\n",
        "# Add to model evaluation section:\n",
        "print(\"\\nMeasuring real-time performance metrics...\")\n",
        "performance_metrics = measure_inference_performance(final_model, X_test_reshaped)\n",
        "\n",
        "print(\"\\nReal-time Performance Metrics:\")\n",
        "print(f\"Average Inference Time: {performance_metrics['avg_inference_time_ms']:.2f} ms\")\n",
        "print(f\"95th Percentile Inference Time: {performance_metrics['p95_inference_time_ms']:.2f} ms\")\n",
        "print(f\"Maximum Inference Time: {performance_metrics['max_inference_time_ms']:.2f} ms\")\n",
        "print(f\"Average Memory Usage: {performance_metrics['avg_memory_usage_mb']:.2f} MB\")\n",
        "print(f\"Peak Memory Usage: {performance_metrics['peak_memory_usage_mb']:.2f} MB\")\n",
        "\n",
        "# Create output directory if it doesn't exist\n",
        "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "base_dir = 'model_outputs'\n",
        "output_dir = f'{base_dir}_{timestamp}'\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "# Save metrics to inference results file\n",
        "with open(os.path.join(output_dir, 'inference_results.txt'), 'w') as f:\n",
        "    f.write('Real-time Performance Metrics:\\n')\n",
        "    for metric, value in performance_metrics.items():\n",
        "        f.write(f\"{metric}: {value:.2f}\\n\")"
      ],
      "metadata": {
        "id": "ouaDlmdHgirv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ffd304ce-61ab-4cd1-9444-2efa1e8976ac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Measuring real-time performance metrics...\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 96ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 105ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 87ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 86ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 94ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 76ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 71ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 74ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 99ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 74ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 89ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 74ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 102ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 103ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 79ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
            "\n",
            "Real-time Performance Metrics:\n",
            "Average Inference Time: 127.32 ms\n",
            "95th Percentile Inference Time: 270.46 ms\n",
            "Maximum Inference Time: 453.35 ms\n",
            "Average Memory Usage: 1196.77 MB\n",
            "Peak Memory Usage: 1200.06 MB\n"
          ]
        }
      ]
    }
  ]
}