{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "82YxkeUCaguA"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "from torch.utils.data import TensorDataset, DataLoader, SubsetRandomSampler, random_split\n",
        "from sklearn.model_selection import KFold\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from itertools import product\n",
        "import time\n",
        "from google.colab import files\n",
        "import os\n",
        "from datetime import datetime\n",
        "import psutil\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from torch.utils.data import random_split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CTMUNCa2Hcot"
      },
      "outputs": [],
      "source": [
        "# Load and prepare data\n",
        "X_train = pd.read_csv(\"UCI_data/train/X_train.txt\", sep='\\s+', header=None)\n",
        "y_train = pd.read_csv(\"UCI_data/train/y_train.txt\", header=None)\n",
        "subject_train = pd.read_csv(\"UCI_data/train/subject_train.txt\", header=None)\n",
        "\n",
        "X_test = pd.read_csv(\"UCI_data/test/X_test.txt\", sep='\\s+', header=None)\n",
        "y_test = pd.read_csv(\"UCI_data/test/y_test.txt\", header=None)\n",
        "subject_test = pd.read_csv(\"UCI_data/test/subject_test.txt\", header=None)\n",
        "\n",
        "print(\"Initial data shapes:\")\n",
        "print(f\"X_train shape: {X_train.shape}\")\n",
        "print(f\"y_train shape: {y_train.shape}\")\n",
        "print(f\"X_test shape: {X_test.shape}\")\n",
        "print(f\"y_test shape: {y_test.shape}\")\n",
        "\n",
        "X_train = X_train.values\n",
        "y_train = y_train.values\n",
        "X_test = X_test.values\n",
        "y_test = y_test.values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xyar4CmxHcsI",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "# Create model architecture function\n",
        "\n",
        "def create_model(input_features, hidden_size1, hidden_size2, dropout_rate, n_classes=6):\n",
        "    return nn.Sequential(\n",
        "        nn.Linear(input_features, hidden_size1),\n",
        "        nn.ReLU(),\n",
        "        nn.Dropout(dropout_rate),\n",
        "        nn.Linear(hidden_size1, hidden_size2),\n",
        "        nn.ReLU(),\n",
        "        nn.Dropout(dropout_rate),\n",
        "        nn.Linear(hidden_size2, n_classes)\n",
        "    )\n",
        "\n",
        "# Create model training function\n",
        "\n",
        "def train_model(model, train_loader, criterion, optimizer, device):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    for inputs, labels in train_loader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        _, predicted = outputs.max(1)\n",
        "        total += labels.size(0)\n",
        "        correct += predicted.eq(labels).sum().item()\n",
        "\n",
        "    return running_loss / len(train_loader), 100. * correct / total\n",
        "\n",
        "# Validation function\n",
        "\n",
        "def validate_model(model, val_loader, criterion, device):\n",
        "    model.eval()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in val_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            running_loss += loss.item()\n",
        "            _, predicted = outputs.max(1)\n",
        "            total += labels.size(0)\n",
        "            correct += predicted.eq(labels).sum().item()\n",
        "\n",
        "    return running_loss / len(val_loader), 100. * correct / total\n",
        "\n",
        "# Cross-validation grid search function\n",
        "\n",
        "def grid_search_cv(X_train, y_train, param_grid, n_folds=5, epochs=30, device='cuda'):\n",
        "    # Convert data to PyTorch tensors\n",
        "    X_tensor = torch.FloatTensor(X_train)\n",
        "    y_tensor = torch.LongTensor(y_train.ravel() - 1)  # Assuming classes start from 1\n",
        "\n",
        "    dataset = TensorDataset(X_tensor, y_tensor)\n",
        "\n",
        "    # Initialize stratified K-fold cross validation\n",
        "    skfold = StratifiedKFold(n_splits=n_folds, shuffle=True, random_state=42)\n",
        "\n",
        "\n",
        "    param_combinations = [dict(zip(param_grid.keys(), v)) for v in product(*param_grid.values())]\n",
        "\n",
        "    results = []\n",
        "\n",
        "    for params in param_combinations:\n",
        "        fold_scores = []\n",
        "        print(f\"\\nTesting parameters: {params}\")\n",
        "\n",
        "        # Perform stratified k-fold cross validation\n",
        "        for fold, (train_idx, val_idx) in enumerate(skfold.split(X_tensor, y_tensor)):\n",
        "            print(f\"\\nFold {fold + 1}/{n_folds}\")\n",
        "\n",
        "            # Create data loaders for train and validation sets\n",
        "            train_sampler = SubsetRandomSampler(train_idx)\n",
        "            val_sampler = SubsetRandomSampler(val_idx)\n",
        "\n",
        "            train_loader = DataLoader(dataset, batch_size=params['batch_size'], sampler=train_sampler)\n",
        "            val_loader = DataLoader(dataset, batch_size=params['batch_size'], sampler=val_sampler)\n",
        "\n",
        "            # Create model\n",
        "            model = create_model(\n",
        "                input_features=561,\n",
        "                hidden_size1=params['hidden_size1'],\n",
        "                hidden_size2=params['hidden_size2'],\n",
        "                dropout_rate=params['dropout_rate']\n",
        "            ).to(device)\n",
        "\n",
        "            # Initialize optimizer and criterion\n",
        "            optimizer = torch.optim.Adam(model.parameters(), weight_decay = params[\"weight_decay\"], lr=params['learning_rate'])\n",
        "            criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "            # Training loop\n",
        "            best_val_acc = 0\n",
        "            patience = 5\n",
        "            counter = 0\n",
        "\n",
        "            for epoch in range(epochs):\n",
        "                train_loss, train_acc = train_model(model, train_loader, criterion, optimizer, device)\n",
        "                val_loss, val_acc = validate_model(model, val_loader, criterion, device)\n",
        "\n",
        "                if val_acc > best_val_acc:\n",
        "                    best_val_acc = val_acc\n",
        "                    counter = 0\n",
        "                else:\n",
        "                    counter += 1\n",
        "\n",
        "                if counter >= patience:\n",
        "                    print(f\"Early stopping at epoch {epoch + 1}\")\n",
        "                    break\n",
        "\n",
        "                if (epoch + 1) % 5 == 0:\n",
        "                    print(f\"Epoch {epoch + 1}/{epochs}\")\n",
        "                    print(f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%\")\n",
        "                    print(f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%\")\n",
        "\n",
        "            fold_scores.append(best_val_acc)\n",
        "\n",
        "        # Calculate mean and std of scores for current parameters\n",
        "        mean_score = np.mean(fold_scores)\n",
        "        std_score = np.std(fold_scores)\n",
        "\n",
        "\n",
        "        result_dict = {\n",
        "            'hidden_size1': params['hidden_size1'],\n",
        "            'hidden_size2': params['hidden_size2'],\n",
        "            'dropout_rate': params['dropout_rate'],\n",
        "            'learning_rate': params['learning_rate'],\n",
        "            'weight_decay': params['weight_decay'],\n",
        "            'batch_size': params['batch_size'],\n",
        "            'mean_val_accuracy': mean_score,\n",
        "            'std_val_accuracy': std_score\n",
        "        }\n",
        "        results.append(result_dict)\n",
        "\n",
        "\n",
        "\n",
        "        print(f\"\\nParameter set complete:\")\n",
        "        print(f\"Mean validation accuracy: {mean_score:.2f}% (Â±{std_score:.2f})\")\n",
        "\n",
        "    results_df = pd.DataFrame(results)\n",
        "\n",
        "    # Add timestamp to filename\n",
        "    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
        "    csv_path = f'gridsearch_results_{timestamp}.csv'\n",
        "\n",
        "    # Save and download\n",
        "    results_df.to_csv(csv_path, index=False)\n",
        "    files.download(csv_path)\n",
        "\n",
        "    print(f\"\\nDownloaded GridSearch results to {csv_path}\")\n",
        "    return results_df, results\n",
        "\n",
        "# Define parameter grid\n",
        "param_grid = {\n",
        "    'learning_rate': [0.001, 0.0005],\n",
        "    'hidden_size1': [50],\n",
        "    'hidden_size2': [20],\n",
        "    'dropout_rate': [0.1, 0.3, 0.5],\n",
        "    'batch_size': [64, 128],\n",
        "    'weight_decay': [0.001, 0.0005]\n",
        "}\n",
        "\n",
        "\n",
        "\n",
        "# Set device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Run grid search\n",
        "results = grid_search_cv(X_train, y_train, param_grid, device=device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z8aHGhgYHc84",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "def set_seeds(seed=42):\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "    np.random.seed(seed)\n",
        "\n",
        "# Plotting function\n",
        "\n",
        "def plot_training_metrics(train_losses, train_accs, val_losses, val_accs):\n",
        "    epochs = range(1, len(train_losses) + 1)\n",
        "\n",
        "    plt.figure(figsize=(12, 5))\n",
        "\n",
        "    # Plot losses\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(epochs, train_losses, label='Training')\n",
        "    plt.plot(epochs, val_losses, label='Validation')\n",
        "    plt.title('Model Loss')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "\n",
        "    # Plot accuracies\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(epochs, train_accs, label='Training')\n",
        "    plt.plot(epochs, val_accs, label='Validation')\n",
        "    plt.title('Model Accuracy')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Accuracy (%)')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Function for inference performance metrics\n",
        "\n",
        "def measure_inference_performance(model, X_test, device, n_runs=100):\n",
        "    latencies = []\n",
        "    memory_usage = []\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        model(torch.FloatTensor(X_test[:1]).to(device))\n",
        "\n",
        "    for _ in range(n_runs):\n",
        "        process = psutil.Process()\n",
        "        memory_usage.append(process.memory_info().rss / 1024 / 1024)\n",
        "\n",
        "        start_time = time.time()\n",
        "        with torch.no_grad():\n",
        "            model(torch.FloatTensor(X_test[:1]).to(device))\n",
        "        latencies.append((time.time() - start_time) * 1000)\n",
        "\n",
        "        if device.type == 'cuda':\n",
        "            torch.cuda.synchronize()\n",
        "\n",
        "    metrics = {\n",
        "        'avg_inference_time_ms': np.mean(latencies),\n",
        "        'std_inference_time_ms': np.std(latencies),\n",
        "        'max_inference_time_ms': np.max(latencies),\n",
        "        'p95_inference_time_ms': np.percentile(latencies, 95),\n",
        "        'avg_memory_usage_mb': np.mean(memory_usage),\n",
        "        'peak_memory_usage_mb': np.max(memory_usage),\n",
        "        'device': device.type\n",
        "    }\n",
        "\n",
        "    return metrics\n",
        "def save_and_download_model_weights(model):\n",
        "\n",
        "    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
        "\n",
        "    model_path = f'model_weights_{timestamp}.pth'\n",
        "    torch.save(model.state_dict(), model_path)\n",
        "\n",
        "    # Download the file\n",
        "    files.download(model_path)\n",
        "    print(f\"Downloaded model weights to {model_path}\")\n",
        "\n",
        "\n",
        "set_seeds(42)\n",
        "\n",
        "batch_size =  128\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Convert to PyTorch tensors\n",
        "X_train_tensor = torch.FloatTensor(X_train)\n",
        "y_train_tensor = torch.LongTensor(y_train.ravel() - 1)\n",
        "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
        "\n",
        "X_test_tensor = torch.FloatTensor(X_test)\n",
        "y_test_tensor = torch.LongTensor(y_test.ravel() - 1)\n",
        "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
        "\n",
        "# Split training data\n",
        "val_size = int(0.2 * len(train_dataset))\n",
        "train_size = len(train_dataset) - val_size\n",
        "train_subset, val_subset = random_split(train_dataset, [train_size, val_size])\n",
        "\n",
        "# Create data loaders\n",
        "train_loader = DataLoader(train_subset, batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_subset, batch_size)\n",
        "test_loader = DataLoader(test_dataset, batch_size)\n",
        "\n",
        "model = create_model(\n",
        "    input_features = 561,\n",
        "    hidden_size1 = 50,\n",
        "    hidden_size2 = 20,\n",
        "    dropout_rate = 0.01\n",
        ").to(device)\n",
        "\n",
        "# Initialize optimizer and criterion\n",
        "optimizer = torch.optim.Adam(model.parameters(), weight_decay = 0.001, lr= 0.001)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Training loop\n",
        "epochs = 20\n",
        "train_losses = []\n",
        "train_accs = []\n",
        "val_losses = []\n",
        "val_accs = []\n",
        "\n",
        "print(\"Training final model...\")\n",
        "for epoch in range(epochs):\n",
        "    # Train\n",
        "    train_loss, train_acc = train_model(model, train_loader, criterion, optimizer, device)\n",
        "    train_losses.append(train_loss)\n",
        "    train_accs.append(train_acc)\n",
        "\n",
        "    # Validate\n",
        "    val_loss, val_acc = validate_model(model, val_loader, criterion, device)\n",
        "    val_losses.append(val_loss)\n",
        "    val_accs.append(val_acc)\n",
        "\n",
        "    if (epoch + 1) % 2 == 0:\n",
        "        print(f\"Epoch {epoch + 1}/{epochs}\")\n",
        "        print(f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%\")\n",
        "        print(f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%\")\n",
        "\n",
        "# Plot training metrics\n",
        "plot_training_metrics(train_losses, train_accs, val_losses, val_accs)\n",
        "\n",
        "\n",
        "print(\"\\nMeasuring real-time performance metrics...\")\n",
        "performance_metrics = measure_inference_performance(model, X_test, device)\n",
        "\n",
        "print(\"\\nReal-time Performance Metrics:\")\n",
        "print(f\"Device: {performance_metrics['device']}\")\n",
        "print(f\"Average Inference Time: {performance_metrics['avg_inference_time_ms']:.2f} ms\")\n",
        "print(f\"Std Dev Inference Time: {performance_metrics['std_inference_time_ms']:.2f} ms\")\n",
        "print(f\"Max Inference Time: {performance_metrics['max_inference_time_ms']:.2f} ms\")\n",
        "print(f\"95th Percentile Inference Time: {performance_metrics['p95_inference_time_ms']:.2f} ms\")\n",
        "print(f\"Average Memory Usage: {performance_metrics['avg_memory_usage_mb']:.2f} MB\")\n",
        "print(f\"Peak Memory Usage: {performance_metrics['peak_memory_usage_mb']:.2f} MB\")\n",
        "\n",
        "# Evaluate on test set\n",
        "test_loss, test_acc = validate_model(model, test_loader, criterion, device)\n",
        "print(f\"\\nFinal test accuracy: {test_acc:.2f}%\")\n",
        "\n",
        "save_and_download_model_weights(model)\n",
        "\n",
        "activities = [\n",
        "    'WALKING',\n",
        "    'WALKING_UPSTAIRS',\n",
        "    'WALKING_DOWNSTAIRS',\n",
        "    'SITTING',\n",
        "    'STANDING',\n",
        "    'LAYING'\n",
        "]\n",
        "\n",
        "# Create confusion matrix\n",
        "model.eval()\n",
        "y_pred = []\n",
        "y_true = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for inputs, labels in test_loader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        outputs = model(inputs)\n",
        "        _, predicted = outputs.max(1)\n",
        "        y_pred.extend(predicted.cpu().numpy())\n",
        "        y_true.extend(labels.cpu().numpy())\n",
        "\n",
        "cm = confusion_matrix(y_true, y_pred)\n",
        "\n",
        "plt.figure(figsize=(12, 10))\n",
        "sns.heatmap(cm,\n",
        "            annot=True,\n",
        "            fmt='d',\n",
        "            cmap='Blues',\n",
        "            xticklabels=activities,\n",
        "            yticklabels=activities)\n",
        "\n",
        "plt.title('Confusion Matrix of Human Activities')\n",
        "plt.xlabel('Predicted Activity')\n",
        "plt.ylabel('True Activity')\n",
        "\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.yticks(rotation=0)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}